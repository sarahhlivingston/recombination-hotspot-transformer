{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPsZT75Dj6M9kbGMg36u7uM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarahhlivingston/recombination-hotspot-transformer/blob/main/hotspot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "DNva2D-ul40m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6cfKeZIbsvJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score, recall_score, precision_score, roc_curve, auc\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mhRPWmoZcYR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plots & Helper Functions"
      ],
      "metadata": {
        "id": "2EiiSMrHX53G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## metrics"
      ],
      "metadata": {
        "id": "8LKAdbQ5YMCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics(y_true, y_pred, threshold=0.5):\n",
        "\n",
        "  #convert to numpy --> isinstance: is this object an instance of this class\n",
        "  if isinstance(y_true, torch.Tensor):\n",
        "    y_true = y_true.cpu().numpy()\n",
        "  if isinstance(y_pred, torch.Tensor):\n",
        "    y_pred = y_pred.cpu().numpy()\n",
        "\n",
        "\n",
        "  y_pred_bin = (y_pred >= threshold).astype(int)\n",
        "  y_true_bin = y_true.astype(int)\n",
        "\n",
        "\n",
        "  metrics = {\n",
        "        \"accuracy\": accuracy_score(y_true_bin, y_pred_bin),\n",
        "        \"f1\": f1_score(y_true_bin, y_pred_bin, average='macro'), ##micro and macro same in binary classification\n",
        "        \"confusion_matrix\": confusion_matrix(y_true_bin, y_pred_bin),\n",
        "        # \"auc_score\": roc_auc_score(y_true_bin, y_pred_bin),\n",
        "        \"auc_score\": roc_auc_score(y_true_bin, y_pred),\n",
        "        \"recall_score\": recall_score(y_true_bin, y_pred_bin),\n",
        "        \"precision_score\": precision_score(y_true_bin, y_pred_bin)\n",
        "    }\n",
        "\n",
        "  return metrics\n",
        "\n"
      ],
      "metadata": {
        "id": "fw5Q8wkhX9w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plots"
      ],
      "metadata": {
        "id": "fgLTKEw2YOfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    #loss\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(history['train_loss'], label='Train Loss')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    #accuracy\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
        "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "LZyIjTG7YFd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_roc(y_true, y_pred_probs, label_name=\"Hotspot\"):\n",
        "    if torch.is_tensor(y_true):\n",
        "        y_true = y_true.cpu().numpy()\n",
        "    if torch.is_tensor(y_pred_probs):\n",
        "        y_pred_probs = y_pred_probs.cpu().numpy()\n",
        "    y_true = y_true.ravel()\n",
        "    y_pred_probs = y_pred_probs.ravel()\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_pred_probs)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='navy', lw=2, label=f'{label_name} (AUC = {roc_auc:.3f})')\n",
        "    plt.plot([0, 1], [0, 1], color='black', lw=1, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"AUC = {roc_auc:.3f}\")"
      ],
      "metadata": {
        "id": "y2CM4aHXjELq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n"
      ],
      "metadata": {
        "id": "o42eeB9ZcKzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## S. cerevisae"
      ],
      "metadata": {
        "id": "wONcDXvGiOXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_hotspot_splits(data, seq_col=\"sequence\", label_col=\"label\",\n",
        "                          test_size=0.2, val_size=0.5, random_state=42):\n",
        "\n",
        "    # Keep only sequence + label col\n",
        "    data = data[[seq_col, label_col]].copy()\n",
        "\n",
        "    train_df, temp_df = train_test_split(\n",
        "        data, test_size=test_size, stratify=data[label_col], random_state=random_state\n",
        "    )\n",
        "\n",
        "    val_df, test_df = train_test_split(\n",
        "        temp_df, test_size=val_size, stratify=temp_df[label_col], random_state=random_state\n",
        "    )\n",
        "\n",
        "    train_seqs = train_df[seq_col].tolist()\n",
        "    val_seqs   = val_df[seq_col].tolist()\n",
        "    test_seqs  = test_df[seq_col].tolist()\n",
        "\n",
        "    train_labels = train_df[label_col].to_numpy(dtype=np.float32)\n",
        "    val_labels   = val_df[label_col].to_numpy(dtype=np.float32)\n",
        "    test_labels  = test_df[label_col].to_numpy(dtype=np.float32)\n",
        "\n",
        "    return train_seqs, train_labels, val_seqs, val_labels, test_seqs, test_labels"
      ],
      "metadata": {
        "id": "xR_bRLkv6egO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/715/yps128_data.csv\")  # columns: ID, sequence, label\n",
        "\n",
        "train_seqs, train_labels, val_seqs, val_labels, test_seqs, test_labels = create_hotspot_splits(data)\n",
        "\n",
        "print(f\"Train: {len(train_seqs)}\\nVal: {len(val_seqs)}\\nTest: {len(test_seqs)}\")"
      ],
      "metadata": {
        "id": "i-avknC85xIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"/content/drive/MyDrive/715/train_labels.npy\", train_labels)\n",
        "np.save(\"/content/drive/MyDrive/715/test_labels.npy\", test_labels)\n",
        "np.save(\"/content/drive/MyDrive/715/val_labels.npy\", val_labels)"
      ],
      "metadata": {
        "id": "EQ9N3KP76va5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## S. paradoxus"
      ],
      "metadata": {
        "id": "utEItMw1iTbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spar_data = pd.read_csv(\"/content/drive/MyDrive/715/paradoxus/spar_data.csv\")\n",
        "spar_labels = spar_data[\"label\"].to_numpy(dtype=np.float32)\n",
        "spar_seqs = spar_data[\"sequence\"].tolist()\n",
        "np.save(\"/content/drive/MyDrive/715/paradoxus/spar_labels.npy\", spar_labels)"
      ],
      "metadata": {
        "id": "tCfuJSV_iXY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## S. mikatae\n"
      ],
      "metadata": {
        "id": "pJIQVKZ7y5Z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smik_data = pd.read_csv(\"/content/drive/MyDrive/715/Smik/smik_data.csv\")\n",
        "smik_labels = smik_data[\"label\"].to_numpy(dtype=np.float32)\n",
        "smik_seqs = smik_data[\"sequence\"].tolist()\n",
        "np.save(\"/content/drive/MyDrive/715/Smik/smik_labels.npy\", smik_labels)"
      ],
      "metadata": {
        "id": "v-SeYKRizLGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## S.  kudriavzevii"
      ],
      "metadata": {
        "id": "mcUu2ZLty9yF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "skud_data = pd.read_csv(\"/content/drive/MyDrive/715/Skud/skud_data.csv\")\n",
        "skud_labels = skud_data[\"label\"].to_numpy(dtype=np.float32)\n",
        "skud_seqs = skud_data[\"sequence\"].tolist()\n",
        "np.save(\"/content/drive/MyDrive/715/Skud/skud_labels.npy\", skud_labels)"
      ],
      "metadata": {
        "id": "g6CJgT5jzjTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXTRACT EMBEDDINGS\n"
      ],
      "metadata": {
        "id": "fWFgnW6Sm8AE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Model\n"
      ],
      "metadata": {
        "id": "RcggU5belPGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"InstaDeepAI/nucleotide-transformer-v2-500m-multi-species\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForMaskedLM.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    output_hidden_states=True\n",
        ")\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "RdK4Sd_1rr4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding Function\n"
      ],
      "metadata": {
        "id": "Hi_Y41650Amp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_yeast_embeddings(sequences, model, tokenizer, batch_size=8, save_path=None, max_length=1000):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    embeddings = []\n",
        "\n",
        "    for i in tqdm(range(0, len(sequences), batch_size), desc=\"Embedding batches\"):\n",
        "        batch_seqs = sequences[i:i+batch_size]\n",
        "\n",
        "        # Tokenize and pad\n",
        "        inputs = tokenizer(batch_seqs,\n",
        "                           return_tensors=\"pt\",\n",
        "                           padding=True,\n",
        "                           truncation=True,\n",
        "                           max_length=max_length).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "            # Average the last 4 layers: shape [batch, seq_len, embed_dim]\n",
        "            hidden_states = torch.stack(outputs.hidden_states[-4:], dim=0).mean(0)\n",
        "\n",
        "            # Masked mean pooling over sequence length\n",
        "            mask = inputs['attention_mask'].unsqueeze(-1)  # [batch, seq_len, 1]\n",
        "            summed = (hidden_states * mask).sum(dim=1)     # [batch, embed_dim]\n",
        "            counts = mask.sum(dim=1).clamp(min=1e-9)       # avoid div by 0\n",
        "            batch_embs = (summed / counts).cpu().numpy()   # [batch, embed_dim]\n",
        "\n",
        "            embeddings.append(batch_embs)\n",
        "\n",
        "    all_embeddings = np.vstack(embeddings)  # [num_sequences, embed_dim]\n",
        "\n",
        "    if save_path:\n",
        "        np.save(save_path, all_embeddings)\n",
        "        print(f\"Saved embeddings to {save_path}\")\n",
        "\n",
        "    return all_embeddings"
      ],
      "metadata": {
        "id": "VXB_cfq2oxGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## S. cerevisae embeddings --> for training"
      ],
      "metadata": {
        "id": "U-3PIxad0GEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_embeddings = get_yeast_embeddings(train_seqs,\n",
        "#                                         model,\n",
        "#                                         tokenizer,\n",
        "#                                         save_path = \"/content/drive/MyDrive/715/train_embeddings.npy\")\n",
        "\n",
        "# test_embeddings = get_yeast_embeddings(test_seqs,\n",
        "#                                        model,\n",
        "#                                        tokenizer,\n",
        "#                                        save_path = \"/content/drive/MyDrive/715/test_embeddings.npy\")\n",
        "\n",
        "# val_embeddings = get_yeast_embeddings(val_seqs,\n",
        "#                                       model,\n",
        "#                                       tokenizer,\n",
        "#                                       save_path = \"/content/drive/MyDrive/715/val_embeddings.npy\")"
      ],
      "metadata": {
        "id": "PM_v53bgpyCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## S. paradoxus embeddings --> evalulation"
      ],
      "metadata": {
        "id": "qw940Q7n0LQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# spar_embeddings = get_yeast_embeddings(spar_seqs,\n",
        "#                                        model,\n",
        "#                                        tokenizer,\n",
        "#                                        save_path = \"/content/drive/MyDrive/715/paradoxus/spar_embeddings.npy\")"
      ],
      "metadata": {
        "id": "Q1fN_0rwlvC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## S. mikatae embeddings --> evaluate"
      ],
      "metadata": {
        "id": "5NVEDiyh0SHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smik_embeddings = get_yeast_embeddings(smik_seqs,\n",
        "                                       model,\n",
        "                                       tokenizer,\n",
        "                                       save_path = \"/content/drive/MyDrive/715/Smik/smik_embeddings.npy\")"
      ],
      "metadata": {
        "id": "VwK6i5u40jHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## S. kudriavzevii --> evaluate\n"
      ],
      "metadata": {
        "id": "TCNvdkbx0Zqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "skud_embeddings = get_yeast_embeddings(skud_seqs,\n",
        "                                       model,\n",
        "                                       tokenizer,\n",
        "                                       save_path = \"/content/drive/MyDrive/715/Skud/skud_embeddings.npy\")"
      ],
      "metadata": {
        "id": "ydne6r4w0q6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SIMPLE CLASSIFIER"
      ],
      "metadata": {
        "id": "1R5xyL0bsCDG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset (embeddings +labels)"
      ],
      "metadata": {
        "id": "tmI_6cviYUxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryDataset(Dataset):\n",
        "    def __init__(self, embeddings_path, labels_path):\n",
        "        embeddings = np.load(embeddings_path)   # float32\n",
        "        labels = np.load(labels_path)\n",
        "\n",
        "        embeddings = np.nan_to_num(embeddings, nan=0.0)\n",
        "        labels = np.nan_to_num(labels, nan=0.0)\n",
        "\n",
        "        self.embeddings = torch.from_numpy(embeddings).float()\n",
        "        self.labels = torch.from_numpy(labels).float()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embeddings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.embeddings[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "8mH_VeprsFMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = BinaryDataset(\"/content/drive/MyDrive/715/train_embeddings.npy\",\n",
        "                               \"/content/drive/MyDrive/715/train_labels.npy\")\n",
        "val_data   = BinaryDataset(\"/content/drive/MyDrive/715/val_embeddings.npy\",\n",
        "                               \"/content/drive/MyDrive/715/val_labels.npy\")\n",
        "test_data  = BinaryDataset(\"/content/drive/MyDrive/715/test_embeddings.npy\",\n",
        "                               \"/content/drive/MyDrive/715/test_labels.npy\")\n",
        "\n",
        "train_dataloader = DataLoader(train_data,\n",
        "                              batch_size = 16,\n",
        "                              shuffle = True,\n",
        "                              drop_last = True)\n",
        "val_dataloader = DataLoader(val_data,\n",
        "                            batch_size = 64,\n",
        "                            shuffle = False)\n",
        "test_dataloader = DataLoader(test_data,\n",
        "                             batch_size = 64,\n",
        "                             shuffle = False)"
      ],
      "metadata": {
        "id": "bwIzV_Hw3-di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## simple classifier model"
      ],
      "metadata": {
        "id": "6Pk0L7-hYchQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)  # shape: [batch_size, 1]"
      ],
      "metadata": {
        "id": "imgziBeb4I6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deeper with Residual connections"
      ],
      "metadata": {
        "id": "Dkbg1pLhfELO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, dim, dropout=0.4):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.BatchNorm1d(dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.BatchNorm1d(dim)\n",
        "        )\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.layers(x)\n",
        "        out += residual  # Residual connection\n",
        "        return self.relu(out)\n",
        "\n",
        "class DeepResClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=1024, output_dim=1, hidden_dim=624, num_layers=3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Input projection\n",
        "        self.input_projection = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4)\n",
        "        )\n",
        "\n",
        "        # residual layers (num_layers)\n",
        "        self.residual_layers = nn.ModuleList([\n",
        "            ResidualBlock(hidden_dim, dropout=0.4) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output head\n",
        "        self.output_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.BatchNorm1d(hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(hidden_dim // 2, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input projection\n",
        "        x = self.input_projection(x)\n",
        "\n",
        "        # Apply residual layers\n",
        "        for residual_block in self.residual_layers:\n",
        "            x = residual_block(x)\n",
        "\n",
        "\n",
        "        return self.output_head(x)"
      ],
      "metadata": {
        "id": "2DS560PBTzyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, dim, dropout=0.4):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.LayerNorm(dim)\n",
        "        )\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.layers(x)\n",
        "        out += residual  # Residual connection\n",
        "        return self.relu(out)\n",
        "\n",
        "class ResClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=1024, output_dim=1, hidden_dim=624, num_layers=3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Input projection\n",
        "        self.input_projection = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4)\n",
        "        )\n",
        "\n",
        "        # residual layers (num_layers)\n",
        "        self.residual_layers = nn.ModuleList([\n",
        "            ResidualBlock(hidden_dim, dropout=0.4) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output head\n",
        "        self.output_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.LayerNorm(hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(hidden_dim // 2, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input projection\n",
        "        x = self.input_projection(x)\n",
        "\n",
        "        # Apply residual layers\n",
        "        for residual_block in self.residual_layers:\n",
        "            x = residual_block(x)\n",
        "\n",
        "        return self.output_head(x)"
      ],
      "metadata": {
        "id": "p9TIj00aekxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, dim, dropout=0.4):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(dim, dim)\n",
        "        self.bn1 = nn.BatchNorm1d(dim)\n",
        "        self.fc2 = nn.Linear(dim, dim)\n",
        "        self.bn2 = nn.BatchNorm1d(dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.relu(self.bn1(self.fc1(x)))\n",
        "        out = self.drop(out)\n",
        "        out = self.relu(self.bn2(self.fc2(out)))\n",
        "        return self.relu(out + residual)\n",
        "\n",
        "class simpleResClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=1024, hidden_dim=624, output_dim=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_projection = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4)\n",
        "        )\n",
        "\n",
        "        self.residual = ResidualBlock(hidden_dim, dropout=0.4)\n",
        "\n",
        "        self.output_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(hidden_dim // 2, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_projection(x)\n",
        "        x = self.residual(x)\n",
        "        return self.output_head(x)   # logits"
      ],
      "metadata": {
        "id": "xNQEC5eVNP1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, dim, dropout=0.4):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.BatchNorm1d(dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.BatchNorm1d(dim)\n",
        "        )\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.layers(x)\n",
        "        out += residual  # Residual connection\n",
        "        return self.relu(out)\n",
        "\n",
        "class noDeepResClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=1024, output_dim=1, hidden_dim=256, num_layers=4):\n",
        "        super().__init__()\n",
        "\n",
        "        # Input projection\n",
        "        self.input_projection = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # residual layers (num_layers)\n",
        "        self.residual_layers = nn.ModuleList([\n",
        "            ResidualBlock(hidden_dim, dropout=0.4) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output head\n",
        "        self.output_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.BatchNorm1d(hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim // 2, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input projection\n",
        "        x = self.input_projection(x)\n",
        "\n",
        "        # Apply residual layers\n",
        "        for residual_block in self.residual_layers:\n",
        "            x = residual_block(x)\n",
        "\n",
        "\n",
        "        return self.output_head(x)"
      ],
      "metadata": {
        "id": "Vc63jkmJZdcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Classfier with Attention"
      ],
      "metadata": {
        "id": "dB85PkNVx_Sd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "  def __init__(self, dim, num_heads=4):\n",
        "    super().__init__()\n",
        "    self.attention = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, batch_first=True)\n",
        "    self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # 1. x = [batch, features]\n",
        "    x = x.unsqueeze(1)    #2. x = [batch, 1, features]\n",
        "    attn_out, _ = self.attention(x, x, x) ## self_attention\n",
        "    x = x + attn_out  ##residual\n",
        "    x = self.norm(x)\n",
        "    return x.squeeze(1) # 3. x = [batch, features]"
      ],
      "metadata": {
        "id": "shbGm8gGyCVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepResAttClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=1024, output_dim=1, hidden_dim=624, num_heads=4, num_attention_layers=2):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.num_attention_layers = num_attention_layers\n",
        "\n",
        "        # Input projection\n",
        "        self.input_projection = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "        # Multiple attention layers\n",
        "        self.attention_layers = nn.ModuleList([\n",
        "            AttentionBlock(hidden_dim, num_heads=num_heads)\n",
        "            for _ in range(num_attention_layers)\n",
        "        ])\n",
        "\n",
        "        # Additional residual blocks between attention layers\n",
        "        self.residual_blocks = nn.ModuleList([\n",
        "            ResidualBlock(hidden_dim)\n",
        "            for _ in range(num_attention_layers - 1)\n",
        "        ])\n",
        "\n",
        "        # Feature refinement, cleans up learned stuff\n",
        "        self.feature_refinement = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            ResidualBlock(hidden_dim)\n",
        "        )\n",
        "        #Classfier head\n",
        "        self.classifier_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.BatchNorm1d(hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
        "            nn.BatchNorm1d(hidden_dim // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim // 4, 1)   # binary logit\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_probs=False):\n",
        "        x = self.input_projection(x)\n",
        "\n",
        "        for i in range(self.num_attention_layers):\n",
        "            x = self.attention_layers[i](x)\n",
        "            if i < self.num_attention_layers - 1:\n",
        "                x = self.residual_blocks[i](x)\n",
        "\n",
        "        x = self.feature_refinement(x)\n",
        "\n",
        "        logits = self.classifier_head(x)\n",
        "\n",
        "        if return_probs:\n",
        "          probs = torch.sigmoid(logits)\n",
        "          return logits, probs\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "lvFkb2H5yDdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Model"
      ],
      "metadata": {
        "id": "gk4oUbxpYgeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### DEFINE MODEL, LOSS & OPTIMIZER\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = noDeepResClassifier(input_dim=1024).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-5)"
      ],
      "metadata": {
        "id": "yG-WXSaB8QMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### DEFINE MODEL, LOSS & OPTIMIZER\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# model = DeepResAttClassifier(input_dim=1024).to(device)\n",
        "# criterion = nn.BCEWithLogitsLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)"
      ],
      "metadata": {
        "id": "Nxm-oWlzzQbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train/Val loop"
      ],
      "metadata": {
        "id": "3aNoTjcMYh7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- setup ----------\n",
        "epochs = 100\n",
        "patience = 10\n",
        "best_val_acc = 0.0\n",
        "counter = 0\n",
        "best_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#mode = min --> when monitoring loss\n",
        "#mode = max --> when monitoring accuracy\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=3, min_lr=1e-6)\n",
        "\n",
        "history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
        "\n",
        "\n",
        "# ---------- training loop ----------\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    all_train_preds = []\n",
        "    all_train_labels = []\n",
        "\n",
        "    for Xb, yb in train_dataloader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(Xb)\n",
        "        loss = criterion(logits, yb.unsqueeze(1))  # ensure same shape\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * Xb.size(0)\n",
        "        all_train_preds.append(logits.detach())\n",
        "        all_train_labels.append(yb)\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader.dataset)\n",
        "    train_logits = torch.cat(all_train_preds)\n",
        "    train_labels = torch.cat(all_train_labels)\n",
        "    train_metrics = metrics(train_labels, torch.sigmoid(train_logits))\n",
        "\n",
        "    # ---------- validation ----------\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    all_val_preds = []\n",
        "    all_val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in val_dataloader:\n",
        "            Xb, yb = Xb.to(device), yb.to(device)\n",
        "            logits = model(Xb)\n",
        "            loss = criterion(logits, yb.unsqueeze(1))\n",
        "            val_loss += loss.item() * Xb.size(0)\n",
        "            all_val_preds.append(logits)\n",
        "            all_val_labels.append(yb)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader.dataset)\n",
        "    val_logits = torch.cat(all_val_preds)\n",
        "    val_labels = torch.cat(all_val_labels)\n",
        "    val_metrics = metrics(val_labels, torch.sigmoid(val_logits))\n",
        "\n",
        "    # ---------- scheduler + early stopping ----------\n",
        "    scheduler.step(val_metrics[\"accuracy\"])\n",
        "\n",
        "    history[\"train_loss\"].append(avg_train_loss)\n",
        "    history[\"val_loss\"].append(avg_val_loss)\n",
        "    history[\"train_acc\"].append(train_metrics[\"accuracy\"])\n",
        "    history[\"val_acc\"].append(val_metrics[\"accuracy\"])\n",
        "\n",
        "    print(f\"Epoch {epoch+1:03d} | \"\n",
        "          f\"TrainLoss {avg_train_loss:.4f} | ValLoss {avg_val_loss:.4f} | \"\n",
        "          f\"TrainAcc {train_metrics['accuracy']:.4f} | ValAcc {val_metrics['accuracy']:.4f}\")\n",
        "\n",
        "    # Early stopping on val acc\n",
        "    if val_metrics[\"accuracy\"] > best_val_acc:\n",
        "        best_val_acc = val_metrics[\"accuracy\"]\n",
        "        best_wts = copy.deepcopy(model.state_dict())\n",
        "        counter = 0\n",
        "        torch.save(best_wts, \"best_binary_model_acc.pth\")\n",
        "        print(f\"New best val acc: {best_val_acc:.4f}\")\n",
        "\n",
        "        torch.save(model.state_dict(), \"/content/drive/MyDrive/715/model_weights/weights.pth\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}, (no improvement for {patience} epochs)\")\n",
        "            break\n",
        "\n",
        "# ---------- restore best weights ----------\n",
        "model.load_state_dict(best_wts)\n",
        "print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
        "print(f\"avg train loss: {avg_train_loss:.4f} | avg val loss: {avg_val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "7CZeIcK9-PQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history)"
      ],
      "metadata": {
        "id": "RsPqtVVYYz-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OLD\n",
        "- bestweights.pth for this older version but performs the best\n",
        "\n",
        "KEEP THIS"
      ],
      "metadata": {
        "id": "cJg4bunlfBID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, dim, dropout=0.4):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.BatchNorm1d(dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.BatchNorm1d(dim)\n",
        "        )\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.layers(x)\n",
        "        out += residual  # Residual connection\n",
        "        return self.relu(out)\n",
        "\n",
        "class v1DeepResClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=1024, output_dim=1, hidden_dim=624, num_layers=3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Input projection\n",
        "        self.input_projection = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4)\n",
        "        )\n",
        "\n",
        "        # residual layers (num_layers)\n",
        "        self.residual_layers = nn.ModuleList([\n",
        "            ResidualBlock(hidden_dim, dropout=0.4) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output head\n",
        "        self.output_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.BatchNorm1d(hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(hidden_dim // 2, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input projection\n",
        "        x = self.input_projection(x)\n",
        "\n",
        "        # Apply residual layers\n",
        "        for residual_block in self.residual_layers:\n",
        "            x = residual_block(x)\n",
        "\n",
        "\n",
        "        return self.output_head(x)"
      ],
      "metadata": {
        "id": "cGnqLyOWgudq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### DEFINE MODEL, LOSS & OPTIMIZER\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = DeepResClassifier(input_dim=1024).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)"
      ],
      "metadata": {
        "id": "_EeMF3zigwaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------- setup ----------\n",
        "epochs = 100\n",
        "patience = 10\n",
        "best_val_acc = 0.0\n",
        "counter = 0\n",
        "best_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#mode = min --> when monitoring loss\n",
        "#mode = max --> when monitoring accuracy\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=3, min_lr=1e-6)\n",
        "\n",
        "history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
        "\n",
        "\n",
        "# ---------- training loop ----------\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    all_train_preds = []\n",
        "    all_train_labels = []\n",
        "\n",
        "    for Xb, yb in train_dataloader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(Xb)\n",
        "        loss = criterion(logits, yb.unsqueeze(1))  # ensure same shape\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * Xb.size(0)\n",
        "        all_train_preds.append(logits.detach())\n",
        "        all_train_labels.append(yb)\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader.dataset)\n",
        "    train_logits = torch.cat(all_train_preds)\n",
        "    train_labels = torch.cat(all_train_labels)\n",
        "    train_metrics = metrics(train_labels, torch.sigmoid(train_logits))\n",
        "\n",
        "    # ---------- validation ----------\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    all_val_preds = []\n",
        "    all_val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in val_dataloader:\n",
        "            Xb, yb = Xb.to(device), yb.to(device)\n",
        "            logits = model(Xb)\n",
        "            loss = criterion(logits, yb.unsqueeze(1))\n",
        "            val_loss += loss.item() * Xb.size(0)\n",
        "            all_val_preds.append(logits)\n",
        "            all_val_labels.append(yb)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader.dataset)\n",
        "    val_logits = torch.cat(all_val_preds)\n",
        "    val_labels = torch.cat(all_val_labels)\n",
        "    val_metrics = metrics(val_labels, torch.sigmoid(val_logits))\n",
        "\n",
        "    # ---------- scheduler + early stopping ----------\n",
        "    scheduler.step(val_metrics[\"accuracy\"])\n",
        "\n",
        "    history[\"train_loss\"].append(avg_train_loss)\n",
        "    history[\"val_loss\"].append(avg_val_loss)\n",
        "    history[\"train_acc\"].append(train_metrics[\"accuracy\"])\n",
        "    history[\"val_acc\"].append(val_metrics[\"accuracy\"])\n",
        "\n",
        "    print(f\"Epoch {epoch+1:03d} | \"\n",
        "          f\"TrainLoss {avg_train_loss:.4f} | ValLoss {avg_val_loss:.4f} | \"\n",
        "          f\"TrainAcc {train_metrics['accuracy']:.4f} | ValAcc {val_metrics['accuracy']:.4f}\")\n",
        "\n",
        "    # Early stopping on val acc\n",
        "    if val_metrics[\"accuracy\"] > best_val_acc:\n",
        "        best_val_acc = val_metrics[\"accuracy\"]\n",
        "        best_wts = copy.deepcopy(model.state_dict())\n",
        "        counter = 0\n",
        "        torch.save(best_wts, \"best_binary_model_acc.pth\")\n",
        "        print(f\"New best val acc: {best_val_acc:.4f}\")\n",
        "\n",
        "        torch.save(model.state_dict(), \"/content/drive/MyDrive/715/model_weights/weights.pth\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}, (no improvement for {patience} epochs)\")\n",
        "            break\n",
        "\n",
        "# ---------- restore best weights ----------\n",
        "model.load_state_dict(best_wts)\n",
        "print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
        "print(f\"avg train loss: {avg_train_loss:.4f} | avg val loss: {avg_val_loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "VDB2Xejng0Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EVALUATE"
      ],
      "metadata": {
        "id": "siFSEpNH1CCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on s. cerevisae test"
      ],
      "metadata": {
        "id": "swtlm42LYndt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "all_test_preds = []\n",
        "all_test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for Xb, yb in test_dataloader:\n",
        "        Xb, yb = Xb.float().to(device), yb.to(device)\n",
        "\n",
        "        # Forward pass → model returns logits only\n",
        "        y_logits = model(Xb)\n",
        "\n",
        "        # Convert logits → probabilities\n",
        "        y_pred = torch.sigmoid(y_logits)\n",
        "\n",
        "        # Store predictions and labels\n",
        "        all_test_preds.append(y_pred.cpu())\n",
        "        all_test_labels.append(yb.cpu())\n",
        "\n",
        "# Concatenate across batches\n",
        "all_test_preds = torch.cat(all_test_preds, dim=0)      # shape (N, 1)\n",
        "all_test_labels = torch.cat(all_test_labels, dim=0)    # shape (N, 1)\n",
        "\n",
        "# Convert to numpy\n",
        "y_scores = all_test_preds.numpy()\n",
        "y_test   = all_test_labels.numpy()\n",
        "\n",
        "print(\"Preds:\", y_scores.shape, \"Labels:\", y_test.shape)"
      ],
      "metadata": {
        "id": "GKtThurwYqKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics = metrics(y_test, all_test_preds)  # DO NOT apply sigmoid again\n",
        "\n",
        "for k, v in test_metrics.items():\n",
        "    if isinstance(v, (float, int)):\n",
        "        print(f\"{k}: {v:.4f}\")\n",
        "    elif isinstance(v, np.ndarray):  ##for confusion matrix\n",
        "        print(f\"{k}:\\n{v}\")\n",
        "    else:\n",
        "        print(f\"{k}: {v}\")"
      ],
      "metadata": {
        "id": "V2Q9zi26Y7IP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_roc(y_test, all_test_preds)"
      ],
      "metadata": {
        "id": "tVCWusQXYwvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaulate on S. paradoxus"
      ],
      "metadata": {
        "id": "iFSqXlMFmVWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spar_data  = BinaryDataset(\"/content/drive/MyDrive/715/paradoxus/spar_embeddings.npy\",\n",
        "                               \"/content/drive/MyDrive/715/paradoxus/spar_labels.npy\")\n",
        "\n",
        "spar_dataloader = DataLoader(spar_data,\n",
        "                              batch_size = 16,\n",
        "                              shuffle = True,\n",
        "                              drop_last = True)"
      ],
      "metadata": {
        "id": "BpIujPKHmkUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = DeepResClassifier(input_dim=1024, output_dim=1)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/715/model_weights/bestweights.pth\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "BFHcaAsYoG1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.__class__.__name__)"
      ],
      "metadata": {
        "id": "5cb_2iqYn4Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "all_test_preds = []\n",
        "all_test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for Xb, yb in spar_dataloader:\n",
        "        Xb, yb = Xb.float().to(device), yb.to(device)\n",
        "\n",
        "        # Forward pass → model returns logits only\n",
        "        y_logits = model(Xb)\n",
        "\n",
        "        # Convert logits → probabilities\n",
        "        y_pred = torch.sigmoid(y_logits)\n",
        "\n",
        "        # Store predictions and labels\n",
        "        all_test_preds.append(y_pred.cpu())\n",
        "        all_test_labels.append(yb.cpu())\n",
        "\n",
        "# Concatenate across batches\n",
        "all_test_preds = torch.cat(all_test_preds, dim=0)      # shape (N, 1)\n",
        "all_test_labels = torch.cat(all_test_labels, dim=0)    # shape (N, 1)\n",
        "\n",
        "# Convert to numpy\n",
        "y_scores = all_test_preds.numpy()\n",
        "y_test   = all_test_labels.numpy()\n",
        "\n",
        "print(\"Preds:\", y_scores.shape, \"Labels:\", y_test.shape)"
      ],
      "metadata": {
        "id": "-CVOLJi0mZko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_roc(y_test, all_test_preds)"
      ],
      "metadata": {
        "id": "sYpQFx02mgSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics = metrics(y_test, all_test_preds)  # DO NOT apply sigmoid again\n",
        "\n",
        "for k, v in test_metrics.items():\n",
        "    if isinstance(v, (float, int)):\n",
        "        print(f\"{k}: {v:.4f}\")\n",
        "    elif isinstance(v, np.ndarray):  ##for confusion matrix\n",
        "        print(f\"{k}:\\n{v}\")\n",
        "    else:\n",
        "        print(f\"{k}: {v}\")"
      ],
      "metadata": {
        "id": "-16PhoD7mi01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on S. mikatae"
      ],
      "metadata": {
        "id": "lsnnrr3u0xYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smik_data = BinaryDataset(\"/content/drive/MyDrive/715/Smik/smik_embeddings.npy\",\n",
        "                               \"/content/drive/MyDrive/715/Smik/smik_labels.npy\")\n",
        "\n",
        "smik_dataloader = DataLoader(smik_data,\n",
        "                              batch_size = 16,\n",
        "                              shuffle = True,\n",
        "                              drop_last = True)"
      ],
      "metadata": {
        "id": "xuzRoG8D1JFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = DeepResClassifier(input_dim=1024, output_dim=1)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/715/model_weights/bestweights.pth\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "Bd4gfWh617fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.__class__.__name__)"
      ],
      "metadata": {
        "id": "pvld9gO62AZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "all_test_preds = []\n",
        "all_test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for Xb, yb in smik_dataloader:\n",
        "        Xb, yb = Xb.float().to(device), yb.to(device)\n",
        "\n",
        "        # Forward pass → model returns logits only\n",
        "        y_logits = model(Xb)\n",
        "\n",
        "        # Convert logits → probabilities\n",
        "        y_pred = torch.sigmoid(y_logits)\n",
        "\n",
        "        # Store predictions and labels\n",
        "        all_test_preds.append(y_pred.cpu())\n",
        "        all_test_labels.append(yb.cpu())\n",
        "\n",
        "# Concatenate across batches\n",
        "all_test_preds = torch.cat(all_test_preds, dim=0)      # shape (N, 1)\n",
        "all_test_labels = torch.cat(all_test_labels, dim=0)    # shape (N, 1)\n",
        "\n",
        "# Convert to numpy\n",
        "y_scores = all_test_preds.numpy()\n",
        "y_test   = all_test_labels.numpy()\n",
        "\n",
        "print(\"Preds:\", y_scores.shape, \"Labels:\", y_test.shape)"
      ],
      "metadata": {
        "id": "wr7DV7bI2FCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_roc(y_test, all_test_preds)"
      ],
      "metadata": {
        "id": "ZDHKJzXL2N2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics = metrics(y_test, all_test_preds)  # DO NOT apply sigmoid again\n",
        "\n",
        "for k, v in test_metrics.items():\n",
        "    if isinstance(v, (float, int)):\n",
        "        print(f\"{k}: {v:.4f}\")\n",
        "    elif isinstance(v, np.ndarray):  ##for confusion matrix\n",
        "        print(f\"{k}:\\n{v}\")\n",
        "    else:\n",
        "        print(f\"{k}: {v}\")"
      ],
      "metadata": {
        "id": "ROlC5REl2RTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on S. kudriavzevii\n"
      ],
      "metadata": {
        "id": "nSjmihax0z85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "skud_data = BinaryDataset(\"/content/drive/MyDrive/715/Skud/skud_embeddings.npy\",\n",
        "                                \"/content/drive/MyDrive/715/Skud/skud_labels.npy\")\n",
        "\n",
        "skud_dataloader = DataLoader(skud_data,\n",
        "                              batch_size = 16,\n",
        "                              shuffle = True,\n",
        "                              drop_last = True)"
      ],
      "metadata": {
        "id": "aQv2Ndp_1bpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = DeepResClassifier(input_dim=1024, output_dim=1)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/715/model_weights/bestweights.pth\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "Gbd5iOnv19fA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.__class__.__name__)"
      ],
      "metadata": {
        "id": "dyM2jfyA2A7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "all_test_preds = []\n",
        "all_test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for Xb, yb in skud_dataloader:\n",
        "        Xb, yb = Xb.float().to(device), yb.to(device)\n",
        "\n",
        "        # Forward pass → model returns logits only\n",
        "        y_logits = model(Xb)\n",
        "\n",
        "        # Convert logits → probabilities\n",
        "        y_pred = torch.sigmoid(y_logits)\n",
        "\n",
        "        # Store predictions and labels\n",
        "        all_test_preds.append(y_pred.cpu())\n",
        "        all_test_labels.append(yb.cpu())\n",
        "\n",
        "# Concatenate across batches\n",
        "all_test_preds = torch.cat(all_test_preds, dim=0)      # shape (N, 1)\n",
        "all_test_labels = torch.cat(all_test_labels, dim=0)    # shape (N, 1)\n",
        "\n",
        "# Convert to numpy\n",
        "y_scores = all_test_preds.numpy()\n",
        "y_test   = all_test_labels.numpy()\n",
        "\n",
        "print(\"Preds:\", y_scores.shape, \"Labels:\", y_test.shape)"
      ],
      "metadata": {
        "id": "c6tCnIaL2JS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_roc(y_test, all_test_preds)"
      ],
      "metadata": {
        "id": "r7UldCaX2Orz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics = metrics(y_test, all_test_preds)  # DO NOT apply sigmoid again\n",
        "\n",
        "for k, v in test_metrics.items():\n",
        "    if isinstance(v, (float, int)):\n",
        "        print(f\"{k}: {v:.4f}\")\n",
        "    elif isinstance(v, np.ndarray):  ##for confusion matrix\n",
        "        print(f\"{k}:\\n{v}\")\n",
        "    else:\n",
        "        print(f\"{k}: {v}\")"
      ],
      "metadata": {
        "id": "AopaOMI52SBy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}